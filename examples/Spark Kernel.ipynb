{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features available with the new kernels\n",
    "\n",
    "By default Jupyter notebook comes with a `Python` kernel. sparkmagic provides two additional kernels that you can use with Jupyter. These are:\n",
    "\n",
    "1. **PySpark** (for applications written in Python). PySpark kernel exposes the Spark programming model to Python\n",
    "2. **Spark** (for applications written in Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## How do I use the new kernels? \n",
    "\n",
    "1. Create a notebook with the new kernels. Click **New**, and then click **PySpark** or **Spark**. \n",
    "![Create notebooks with new kernels](https://mysstorage.blob.core.windows.net/notebookimages/overview/jupyter-kernels.png \"Create notebooks with new kernels\") \n",
    "2. This should open a new notebook with the kernel you selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Notebook setup\n",
    "\n",
    "When using Spark kernel notebooks, there is no need to create a SparkContext or a HiveContext; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n",
    "- SparkContext (sc)\n",
    "- HiveContext (sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everytime you run a cell, your web browser window title will show a **(Busy)** status along with the notebook title. You will also see a solid circle next to the **Spark** text in the top-right corner. After the job completes, this will change to a hollow circle.\n",
    "\n",
    "![Status of a Jupyter notebook job](https://mysstorage.blob.core.windows.net/notebookimages/overview/HDI.Spark.Jupyter.Job.Status.Spark.Kernel.png \"Status of a Jupyter notebook job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Spark magics \n",
    "\n",
    "The Spark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (e.g. `%%MAGIC` <args>). The magic command must be the first word in a code cell and allow for multiple lines of content. You can’t put comments before a cell magic.\n",
    "\n",
    "For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help magic (%%help)\n",
    "\n",
    "This magic gives you information about the different supported magics in Spark kernel and the usage for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the sqlContext.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a SQL query that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session information (%%info)\n",
    "\n",
    "Livy is an open source REST server for Spark. When you execute a code cell in a Spark notebook, it creates a Livy session to execute your code. You can use the `%%info` magic to display the current Livy session information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>46</td><td>application_1466020664423_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://localhost/yarnui/hn/proxy/application_1466020664423_0016/\">Link</a></td><td><a target=\"_blank\" href=\"https://localhost/yarnui/10.0.0.10/node/containerlogs/container_e04_1466020664423_0016_01_000001/spark\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session configuration (%%configure)\n",
    " \n",
    "Use the `%%configure` magic to configure new or existing Livy sessions.\n",
    "* If a session is already running, you can change the configuration by using the `-f` argument with `%%configure` magic. This will delete the current session and recreate it with the applied configurations. If you don't provide the `-f` argument, an error will be displayed and no configuration changes will be applied.\n",
    "* If you haven't already started the session, then the `-f` argument is not mandatory. Even if you use it with a session that you are just creating, it will not delete any currently running sessions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "These are some session attributes that can be used for configuration \n",
    "- **\"name\"**: Name of the application\n",
    "- **\"driverMemory\"**: Memory for driver (e.g. 1000M, 2G) \n",
    "- **\"executorMemory\"**: Memory for executor (e.g. 1000M, 2G) \n",
    "\n",
    "For more attributes for session configuration see <a href=\"https://github.com/cloudera/livy/tree/6fe1e80cfc72327c28107e0de20c818c1f13e027#post-sessions\" target=\"_blank\"> the Livy documentation</a>.\n",
    "\n",
    "> **TIP**: The application name should start with `remotesparkmagics` to allow sessions to get automatically cleaned up if an error happened. If you provide a name that does not start with `remotesparkmagics` it will not result in an error but the cleanup won't occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'executorCores': 4, u'kind': 'spark', u'executorMemory': u'4G', u'name': u'remotesparkmagics-sample'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>46</td><td>application_1466020664423_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://localhost/yarnui/hn/proxy/application_1466020664423_0016/\">Link</a></td><td><a target=\"_blank\" href=\"https://localhost/yarnui/10.0.0.10/node/containerlogs/container_e04_1466020664423_0016_01_000001/spark\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\"name\":\"remotesparkmagics-sample\", \"executorMemory\": \"4G\", \"executorCores\":4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Automatic visualization of queries \n",
    "\n",
    "The Pyspark kernel automatically visualizes the output of SQL (HiveQL) queries. You are given the option to choose between several different types of visualizations:\n",
    "- Table\n",
    "- Pie\n",
    "- Line \n",
    "- Area\n",
    "- Bar\n",
    "\n",
    ">**TIP**: When you perform a SQL query, the number of rows of data that will be included in the result data set will be limited by default to 2500 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL magic (%%sql)\n",
    "\n",
    "The Spark kernel supports easy inline HiveQL queries against the `sqlContext`. The (`-o VARIABLE_NAME`) argument persists the output of the SQL query as a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\" target=\"_blank\">Pandas dataframe</a> on the Jupyter server (e.g. `-o query1` in the example below). This means it'll be available in the local mode which will be explained later. The output will be automatically visualized after you run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n",
      "Creating HiveContext as 'sqlContext'\n"
     ]
    }
   ],
   "source": [
    "%%sql -o query1\n",
    "SELECT 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![widget](images/widget.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `-o`, a number of other configuration parameters for SQL queries are available (as described in the `%%help` output above). These include:\n",
    "\n",
    "* `-q`. This causes the magic to return `None`, turning off visualizations for that cell. If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o VARIABLE`. If you want to turn off visualizations without capturing the results (e.g. for running a SQL query with side effects, like a `CREATE TABLE` statement), just use `-q` without specifying a `-o` argument.\n",
    "\n",
    "> Remember that the kernel by default limits the output of a SQL query to 2500 rows. If you want to adjust this default behavior, use these options.\n",
    "\n",
    "* `-m METHOD`, where `METHOD` is either `take` or `sample` (defaults to `take`). If the method is `take`, the kernel will take elements from the top of the result data set; if the method is `sample`, the kernel will randomly sample elements of the data set according to `-r`, described below.\n",
    "\n",
    "* `-r FRACTION`, where `FRACTION` is some floating-point number between 0.0 and 1.0. If the sample method for the SQL query is `sample`, then the kernel will randomly sample this fraction of the elements of the result set for you; e.g. if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows will be randomly sampled.\n",
    "\n",
    "* `-n MAXROWS`, where `MAXROWS` is some integer. The kernel will limit the number of output rows to `MAXROWS`. If `MAXROWS` is a negative number such as `-1`, then the number of rows in the result set will not be limited.\n",
    "\n",
    "> **WARNING**: Be careful with the `-n` option, as it is possible to cause the Jupyter server to run out of memory if you try to collect too many result rows. We recommend only using `-n -1` if you are certain that the result dataset will not be too large.\n",
    "\n",
    "As a final example, this SQL query randomly samples 10% of the rows in the **hivesampletable**, limits the size of the result set to 500, and saves the output into a dataframe called `query2` without doing any visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql -q -m sample -r 0.1 -n 500 -o query2 \n",
    "SELECT * FROM hivesampletable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running locally (%%local)\n",
    "\n",
    "You can use the `%%local` magic to run your code locally on the Jupyter server without going to Spark. Here's a typical use case for this scenario. \n",
    "\n",
    "By default, the output of any code snippet that you run from a Jupyter notebook is available within the context of the session that is persisted on the worker nodes. However, if you want to save a trip to the worker nodes for every computation, and all the data that you need for your computation is available locally on the Jupyter server node (which is the headnode), you can use the `%%local` magic to run the code snippet on the Jupyter server. Typically, you would use `%%local` magic in conjunction with the `%%sql` magic with `-o` parameter. The `-o` parameter would persist the output of the SQL query locally and then `%%local` magic would trigger the next set of code snippet to run locally against the output of the SQL queries that is persisted locally.\n",
    "\n",
    "> **TIPS**: \n",
    "> * Working against locally persisted data is especially useful when you want the flexibility of using the visual representation libraries such as **matplotlib**. If you work directly against the data on the remote worker nodes, the output received through Livy is always text that limits the options of visual representation.\n",
    "\n",
    "\n",
    "> * Remember that SQL queries, by default, limit the number of result rows to 2500 -- and that it is possible to get OOM errors if you accidentally collect too many results from your SQL query. Therefore, if your dataset is large, it is considered a best practice to do your computation or number-crunching on the cluster or in the SQL query rather than in local mode.\n",
    "\n",
    "When you use `%%local` all subsequent lines in the cell will be executed locally. The code in the cell must be valid Python code.\n",
    "\n",
    "This code block prints the length of the result set of the dataframe `query2` -- remember, we used the `-n 500` option to limit the size of the dataframe to 500.\n",
    "\n",
    "> **TIP**: Remember that even if you're working in the Spark kernel, you can only use Python in local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "%%local  \n",
    "print(len(query2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session logs (%%logs)\n",
    "\n",
    "You can get the logs of your current Livy session to debug any issues you encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/09 23:43:28 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n",
      "16/06/09 23:43:28 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n",
      "16/06/09 23:43:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/06/09 23:43:30 INFO TimelineClientImpl: Timeline service address: http://localhost:8188/ws/v1/timeline/\n",
      "16/06/09 23:43:30 INFO Client: Requesting a new application from cluster with 10 NodeManagers\n",
      "16/06/09 23:43:30 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (25600 MB per container)\n",
      "16/06/09 23:43:30 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "16/06/09 23:43:30 INFO Client: Setting up container launch context for our AM\n",
      "16/06/09 23:43:30 INFO Client: Setting up the launch environment for our AM container\n",
      "16/06/09 23:43:30 INFO Client: Preparing resources for our AM container\n",
      "16/06/09 23:43:31 INFO MetricsConfig: loaded properties from hadoop-metrics2-azure-file-system.properties\n",
      "16/06/09 23:43:31 INFO WasbAzureIaasSink: Init starting.\n",
      "16/06/09 23:43:31 INFO AzureIaasSink: Init starting. Initializing MdsLogger.\n",
      "16/06/09 23:43:31 INFO AzureIaasSink: Init completed.\n",
      "16/06/09 23:43:31 INFO WasbAzureIaasSink: Init completed.\n",
      "16/06/09 23:43:31 INFO MetricsSinkAdapter: Sink azurefs2 started\n",
      "16/06/09 23:43:31 INFO MetricsSystemImpl: Scheduled snapshot period at 60 second(s).\n",
      "16/06/09 23:43:31 INFO MetricsSystemImpl: azure-file-system metrics system started\n",
      "16/06/09 23:43:31 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/python/lib/pyspark.zip -> wasb://MYSTORAGE@store.blob.core.windows.net/user/spark/.sparkStaging/application_1464100251524_0001/pyspark.zip\n",
      "16/06/09 23:43:32 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/python/lib/py4j-0.8.2.1-src.zip -> wasb://MYSTORAGE@store.blob.core.windows.net/user/spark/.sparkStaging/application_1464100251524_0001/py4j-0.8.2.1-src.zip\n",
      "16/06/09 23:43:32 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/conf/hive-site.xml -> wasb://MYSTORAGE@store.blob.core.windows.net/user/spark/.sparkStaging/application_1464100251524_0001/hive-site.xml\n",
      "16/06/09 23:43:33 INFO Client: Uploading resource file:/usr/hdp/2.3.3.1-7/spark/python/lib/pyspark.zip -> wasb://MYSTORAGE@store.blob.core.windows.net/user/spark/.sparkStaging/application_1464100251524_0001/pyspark.zip\n",
      "16/06/09 23:43:33 INFO Client: Uploading resource file:/usr/hdp/2.3.3.1-7/spark/python/lib/py4j-0.8.2.1-src.zip -> wasb://MYSTORAGE@store.blob.core.windows.net/user/spark/.sparkStaging/application_1464100251524_0001/py4j-0.8.2.1-src.zip\n",
      "16/06/09 23:43:34 INFO Client: Uploading resource file:/tmp/spark-1c0a5560-2256-4f63-ba1f-c0dfd25dc24b/__spark_conf__324896021270563710.zip -> wasb://MYSTORAGE@store.blob.core.windows.net/user/spark/.sparkStaging/application_1464100251524_0001/__spark_conf__324896021270563710.zip\n",
      "16/06/09 23:43:34 WARN Client: spark.yarn.am.extraJavaOptions will not take effect in cluster mode\n",
      "16/06/09 23:43:34 INFO SecurityManager: Changing view acls to: spark\n",
      "16/06/09 23:43:34 INFO SecurityManager: Changing modify acls to: spark\n",
      "16/06/09 23:43:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)\n",
      "16/06/09 23:43:34 INFO Client: Submitting application 1 to ResourceManager\n",
      "16/06/09 23:43:35 INFO YarnClientImpl: Submitted application application_1464100251524_0001\n",
      "16/06/09 23:43:36 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:36 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1465515814875\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1464100251524_0001/\n",
      "\t user: spark\n",
      "16/06/09 23:43:37 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:38 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:39 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:40 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:41 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:42 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:43 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:44 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:45 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:46 INFO Client: Application report for application_1464100251524_0001 (state: ACCEPTED)\n",
      "16/06/09 23:43:47 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:47 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 10.0.0.4\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1465515814875\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1464100251524_0001/\n",
      "\t user: spark\n",
      "16/06/09 23:43:48 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:49 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:50 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:51 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:52 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:53 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:54 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:55 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:56 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:57 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:58 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:43:59 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:00 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:01 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:02 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:03 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:04 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:05 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:06 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:07 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:08 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:09 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:10 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:11 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:12 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:13 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:14 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:15 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:16 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:17 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:18 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:19 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:20 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:21 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:22 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:23 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:24 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)\n",
      "16/06/09 23:44:25 INFO Client: Application report for application_1464100251524_0001 (state: RUNNING)"
     ]
    }
   ],
   "source": [
    "%%logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete session (%%delete)\n",
    "\n",
    "Use '`%%delete -f -s <session #>`' to delete a session given its session ID. Note that you cannot delete the session that is initiated for the kernel itself. Another notebook might go into an error state, since notebooks are supposed to manage sessions by themselves, and all work will be lost on that session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions cleanup (%%cleanup)\n",
    "\n",
    "Use '`%%cleanup -f`' magic to delete all of the sessions for this cluster, including this notebook's session.\n",
    "The force flag `-f` is mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty Print Spark Dataframes (%%pretty)\n",
    "\n",
    "Use '`%%pretty`' magic to display a Scala Spark dataframe as a HTML formatted table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>col1</th><th>col2</th><th>col3</th></tr><tr><td>28</td><td>44</td><td>36</td></tr><tr><td>16</td><td>41</td><td>72</td></tr><tr><td>27</td><td>14</td><td>45</td></tr></table><br /><pre></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%pretty\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
